{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Data from the Web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start importing all the libraries that we are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup # if you don't have this library, \n",
    "# install it. The homework instructions tell us to use it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the base of our futures requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://isa.epfl.ch/imoniteur_ISAP/!GEDPUBLICREPORTS.filter?ww_i_reportModel=133685247 200'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payloadform = {'ww_i_reportModel': '133685247'}\n",
    "isa_baseurl = 'http://isa.epfl.ch/imoniteur_ISAP/!GEDPUBLICREPORTS'\n",
    "isa_formurl= isa_baseurl + \".filter\"\n",
    "r = requests.get(isa_formurl, params=payloadform)\n",
    "s = r.url + \" \" + str(r.status_code)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Informatique'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "inputs = soup.select('form input') # extraemos una lista con todos los \n",
    "# inputs del form ,los 4 primeros inputs no nos valen\n",
    "inputs[5]['name'] # nombre de la opcion\n",
    "tags=inputs[6].select('option') # extraemos las posibles opcicones \n",
    "# de los inputs \n",
    "tags[1].contents # mostramos el contenido o texto dentro de una opcion\n",
    "tags[1]['value'] # valor que meter en nuestra peticiÃ³n rest (de cada opcion)\n",
    "inputs[5].select('option')[9].contents[0] # extraemos los nombres de los inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data that we have just obtain, we will make the request petitions with a loops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "payloadfix = {'ww_x_GPS': '-1', 'ww_i_reportModel': '133685247', 'ww_i_reportModelXsl': '133685270', 'ww_x_UNITE_ACAD': '','ww_x_PERIODE_ACAD': '', 'ww_x_PERIODE_PEDAGO': '','ww_x_HIVERETE': '' }\n",
    "isa_dataurl = isa_baseurl + \".html\"\n",
    "# We have to make a loop here changing the payloadfix values. (only the next ones)\n",
    "payloadfix['ww_x_UNITE_ACAD'] = inputs[5].select('option')[9]['value']\n",
    "#payloadfix['ww_x_PERIODE_ACAD'] = inputs[6].select('option')[10]['value']#decrement [10]==2007\n",
    "#payloadfix['ww_x_PERIODE_PEDAGO'] = inputs[7].select('option')[1]['value']#[7]==bach6\n",
    "#payloadfix\n",
    "#r = requests.get(isa_dataurl, params=payloadfix)\n",
    "#r.url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here we will need to parse the html using BeautifulSoup library.\n",
    "# The data that we want comes in a html table, won't be really hard to parse it.\n",
    "#Data = BeautifulSoup(r.text, 'html.parser') \n",
    "\n",
    "\n",
    "def extractTable(year,sem):\n",
    "    payloadfix['ww_x_PERIODE_ACAD'] = inputs[6].select('option')[year]['value']\n",
    "    payloadfix['ww_x_PERIODE_PEDAGO'] = inputs[7].select('option')[sem]['value']#[7]==bach6\n",
    "    if(sem==1):\n",
    "        payloadfix['ww_x_HIVERETE'] = inputs[8].select('option')[1]['value']\n",
    "    else:\n",
    "        payloadfix['ww_x_HIVERETE'] = inputs[8].select('option')[2]['value']\n",
    "    \n",
    "    r = requests.get(isa_dataurl, params=payloadfix)\n",
    "    Data = BeautifulSoup(r.text, 'html.parser') \n",
    "    \"\"\"Extract the table from HTML into a pandas Dataframe.\"\"\"\n",
    "    BStable = BeautifulSoup(r.text, 'html.parser').find('table')\n",
    "    \n",
    "    BSRows = BStable.findAll('tr')\n",
    "    # BSrows[0] is just the title of the table, so we ignore it.\n",
    "    headerBSRow = BSRows[1]\n",
    "    headers = [th.string for th in headerBSRow]\n",
    "    # We drop the last column, because it's always empty (originally it is used to make the HTML table look better).\n",
    "    rows = [[tr.string for tr in BSRow.findAll('td')[:-1]] for BSRow in BSRows[2:]]\n",
    "    return pd.DataFrame(data=rows, columns=headers)[headers[-1]]\n",
    "year=10\n",
    "semester=1\n",
    "data=[]\n",
    "i=0\n",
    "while(year>0):\n",
    "#    if(semester==1 and year<3):\n",
    "#        semester=7\n",
    "#        continue\n",
    "    course=extractTable(year,semester)\n",
    "    course.name=str(i)\n",
    "    data.append(course)\n",
    "    if(semester==1):\n",
    "        semester=7\n",
    "    else:\n",
    "        year-=1\n",
    "        semester=1\n",
    "    i+=1\n",
    "n=0\n",
    "df_beg={}\n",
    "df_end={}\n",
    "index=0\n",
    "print('creating df')\n",
    "for i in data:\n",
    "    if(index%2==0):\n",
    "        if(index/2>8):\n",
    "            index+=1\n",
    "            continue\n",
    "        df_beg[str(int(index/2))]=i\n",
    "        print('num:',int(index/2),' len:',len(i))\n",
    "    else:\n",
    "        df_end[str(int((index+1)/2-1))]=i\n",
    "    index+=1\n",
    "index=0\n",
    "index2=1\n",
    "aux=0\n",
    "while(index<len(df_beg)-1):\n",
    "    while(index2<len(df_beg)):\n",
    "        for i in df_beg[str(index2)].values:\n",
    "            if i in df_beg[str(index)].values:\n",
    "                #print(i)\n",
    "                found=df_beg[str(index2)][df_beg[str(index2)]==i] #give us the subserie found\n",
    "                found=found.index[0]#get index from serie\n",
    "                df_beg[str(index2)]=df_beg[str(index2)].drop(found)\n",
    "        index2+=1\n",
    "    index+=1\n",
    "    index2=index+1\n",
    "df_beg=pd.DataFrame(df_beg)\n",
    "df_end=pd.DataFrame(df_end)\n",
    "print('ending df')\n",
    "index=0\n",
    "index2=2\n",
    "#dict_sci={}\n",
    "#while(index<len(df_beg) and index2<len(df_end)):\n",
    "#print(type(df_beg[str(index)]))\n",
    "three_yrs=0\n",
    "four_yrs=0\n",
    "five_yrs=0\n",
    "while(index<len(df_beg)):\n",
    "    if(index2==10):\n",
    "        index+=1\n",
    "        index2=index+1\n",
    "    if(index2==10):\n",
    "        break\n",
    "    for i in df_end[str(index2)].values:\n",
    "        if i in df_beg[str(index)].values:\n",
    "            years=index2-index+1\n",
    "            if years==3:        \n",
    "                three_yrs+=1\n",
    "            elif years==4:\n",
    "                four_yrs+=1\n",
    "            elif years==5:\n",
    "                five_yrs+=1\n",
    "    index2+=1\n",
    "print(\"3 years: \",three_yrs,\"\\t4 years: \",four_yrs,\"\\t5 years:\",five_yrs)\n",
    "\n",
    "#a=set(df_beg[str(index)].values)\n",
    "#b=set(df_end[str(index)].values)\n",
    "#c= a&b\n",
    "#print(c)\n",
    "#print(df_beg[str(index)].values&df_end[str(index2)].values)\n",
    "    \n",
    "#if '179383' in df_beg['1'].values:\n",
    "#    print('existe fuaaa')\n",
    "#    df_beg=df_beg[df_beg['1']!='179383']\n",
    "#print(len(df_beg['1']))\n",
    "\n",
    "#df_end\n",
    "#type(extractTable(year,semester))\n",
    "#print(data)\n",
    "#extractTable(year,semester).values[0]\n",
    "#print(Data.html.body.table('No Sciper'))\n",
    "#table = Data.html.body.table\n",
    "#i=10# elementos td del html\n",
    "#years=10\n",
    "#sciphers_bach1_years=[]\n",
    "#sciphers_bach6_years=[]\n",
    "#sciphers=[]\n",
    "#semester=1\n",
    "#n=0#sciphersrows\n",
    "#print('jjjj')\n",
    "\n",
    "#while(years>0):\n",
    "#    payloadfix['ww_x_PERIODE_ACAD'] = inputs[6].select('option')[years]['value']#decrement [10]==2007\n",
    "#    payloadfix['ww_x_PERIODE_PEDAGO'] = inputs[7].select('option')[semester]['value']#[7]==bach6\n",
    "#    r = requests.get(isa_dataurl, params=payloadfix)\n",
    "#    Data = BeautifulSoup(r.text, 'html.parser') \n",
    "#    table = Data.html.body.table\n",
    "#\n",
    "#    for indX,row in enumerate(table.find_all('td')):\n",
    "#        if(indX==i):\n",
    "#            sciphers.insert(n,row.text)\n",
    "#            i+=12\n",
    "#            n+=1\n",
    "#    if(semester==1):\n",
    "#        sciphers_bach1_years.insert(abs(years-10),sciphers)\n",
    "#        years-=1\n",
    "#        semester=1\n",
    "#    else: \n",
    "#        sciphers_bach6_years.insert(abs(years-10),sciphers)\n",
    "#        semester=1\n",
    "#    n=0\n",
    "#    i=10\n",
    "#    sciphers=[]\n",
    "    \n",
    "#print(sciphers_years[9])\n",
    "#sc_bach1={'No Scipher_2007_Bach1':pd.Series(sciphers_bach1_years[0]),'No Scipher_2008_Bach1':pd.Series(sciphers_bach1_years[1]),\n",
    "#   'No Scipher_2009_Bach1':pd.Series(sciphers_bach1_years[2]),'No Scipher_2010_Bach1':pd.Series(sciphers_bach1_years[3]),\n",
    "#   'No Scipher_2011_Bach1':pd.Series(sciphers_bach1_years[4]),'No Scipher_2012_Bach1':pd.Series(sciphers_bach1_years[5]),\n",
    "#   'No Scipher_2013_Bach1':pd.Series(sciphers_bach1_years[6]),'No Scipher_2014_Bach1':pd.Series(sciphers_bach1_years[7]),\n",
    "#   'No Scipher_2015_Bach1':pd.Series(sciphers_bach1_years[8]),'No Scipher_2016_Bach1':pd.Series(sciphers_bach1_years[9])}\n",
    "#print('jjjj')\n",
    "#sc_bach6={'No Scipher_2007_Bach6':pd.Series(sciphers_bach6_years[0]),'No Scipher_2008_Bach6':pd.Series(sciphers_bach6_years[1]),\n",
    "#   'No Scipher_2009_Bach6':pd.Series(sciphers_bach6_years[2]),'No Scipher_2010_Bach6':pd.Series(sciphers_bach6_years[3]),\n",
    "#   'No Scipher_2011_Bach6':pd.Series(sciphers_bach6_years[4]),'No Scipher_2012_Bach6':pd.Series(sciphers_bach6_years[5]),\n",
    "#   'No Scipher_2013_Bach6':pd.Series(sciphers_bach6_years[6]),'No Scipher_2014_Bach6':pd.Series(sciphers_bach6_years[7]),\n",
    "#   'No Scipher_2015_Bach6':pd.Series(sciphers_bach6_years[8]),'No Scipher_2016_Bach6':pd.Series(sciphers_bach6_years[9])}\n",
    "#print(sciphers_bach1_years[0])\n",
    "#bach1=pd.DataFrame(sc_bach1)\n",
    "#bach6=pd.DataFrame(sc_bach6)\n",
    "#bach6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
